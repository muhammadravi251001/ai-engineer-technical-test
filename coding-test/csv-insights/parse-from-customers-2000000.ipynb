{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b86b8646",
   "metadata": {},
   "source": [
    "**Main Question**: Please parse large CSV, `customers-2000000.csv` and keep the memory low.\n",
    "\n",
    "*Assumptions*:\n",
    "* In this question, I just need to parse the CSV, I don't need to do EDA like previous question.\n",
    "* I try to experimenting on the my proposed technique to parse large CSV and compare them based on the memory usage. Lower is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbaac8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42b2a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"./data/customers-2000000.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a79986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage(obj, framework=\"pandas\"):\n",
    "    if framework == \"pandas\":\n",
    "        # Pandas DataFrame\n",
    "        return obj.memory_usage(deep=True).sum() / (1024**2)\n",
    "    \n",
    "    elif framework == \"csv\":\n",
    "        # List or dict from csv module\n",
    "        return sys.getsizeof(obj) / (1024**2)\n",
    "    \n",
    "    elif framework == \"polars\":\n",
    "        # Polars DataFrame\n",
    "        return obj.estimated_size() / (1024**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a9f7a2",
   "metadata": {},
   "source": [
    "# Method 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d73c4d",
   "metadata": {},
   "source": [
    "Using the original `read_csv` from `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37de06a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas original read_csv: 1506.04 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(FILE_PATH)\n",
    "memory = get_memory_usage(df, framework=\"pandas\")\n",
    "print(f\"Pandas original read_csv: {memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e620ef3",
   "metadata": {},
   "source": [
    "# Method 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d1a4f3",
   "metadata": {},
   "source": [
    "I use chunking method for the parser. Although the total memory is the same as original method, with chunking I can get significantly lower memory usage per chunk and do some preprocessing within the chunk to compile the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e68f92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk memory: 75.30 MB\n",
      "Chunk memory: 75.29 MB\n",
      "Chunk memory: 75.30 MB\n",
      "Chunk memory: 75.31 MB\n",
      "Chunk memory: 75.30 MB\n",
      "Chunk memory: 75.30 MB\n",
      "Chunk memory: 75.31 MB\n",
      "Chunk memory: 75.31 MB\n",
      "Chunk memory: 75.31 MB\n",
      "Chunk memory: 75.30 MB\n",
      "Chunk memory: 75.31 MB\n",
      "Chunk memory: 75.31 MB\n",
      "Chunk memory: 75.30 MB\n",
      "Chunk memory: 75.30 MB\n",
      "Chunk memory: 75.29 MB\n",
      "Chunk memory: 75.29 MB\n",
      "Chunk memory: 75.31 MB\n",
      "Chunk memory: 75.30 MB\n",
      "Chunk memory: 75.30 MB\n",
      "Chunk memory: 75.30 MB\n",
      "Pandas chunking total memory: 1506.04 MB\n"
     ]
    }
   ],
   "source": [
    "chunksize = 100_000\n",
    "total_mem_chunk = 0\n",
    "for chunk in pd.read_csv(FILE_PATH, chunksize=chunksize):\n",
    "    print(f\"Chunk memory: {get_memory_usage(chunk, framework='pandas'):.2f} MB\")\n",
    "    total_mem_chunk += get_memory_usage(chunk, framework=\"pandas\")\n",
    "\n",
    "print(f\"Pandas chunking total memory: {total_mem_chunk:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea24575f",
   "metadata": {},
   "source": [
    "# Method 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a58bd",
   "metadata": {},
   "source": [
    "I change the `dtype` from the previous `df`.\n",
    "\n",
    "* `category` → for strings with multiple duplicates, significantly saving memory.\n",
    "* `int32` → sufficient for indexes up to 2 million, more efficient than `int64`. I can't use `int8` or `int16` since it's smaller than 2 million.\n",
    "* `object` → for unique columns like email/phone, as categories don't help much here.\n",
    "\n",
    "So, `int64` changed into `int32` and some `object` is changed into `category` if it's more likely to be duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a45d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas optimized dtype: 1010.90 MB\n"
     ]
    }
   ],
   "source": [
    "dtype_map = {\n",
    "    \"Index\": \"int32\",\n",
    "    \"Customer Id\": \"category\",\n",
    "    \"First Name\": \"category\",\n",
    "    \"Last Name\": \"category\",\n",
    "    \"Company\": \"category\",\n",
    "    \"City\": \"category\",\n",
    "    \"Country\": \"category\",\n",
    "    \"Phone 1\": \"object\",                # Unique values, so keep as object\n",
    "    \"Phone 2\": \"object\",                # Unique values, so keep as object\n",
    "    \"Email\": \"object\",                  # Unique values, so keep as object\n",
    "    \"Subscription Date\": \"object\",      # Unique values, so keep as object\n",
    "    \"Website\": \"category\"\n",
    "}\n",
    "\n",
    "df_opt = pd.read_csv(FILE_PATH, dtype=dtype_map)\n",
    "memory_opt = get_memory_usage(df_opt, framework=\"pandas\")\n",
    "print(f\"Pandas optimized dtype: {memory_opt:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c5b932",
   "metadata": {},
   "source": [
    "# Method 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47548f1",
   "metadata": {},
   "source": [
    "Manual approach using `csv` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15638e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV module total memory estimate: 1220.70 MB\n"
     ]
    }
   ],
   "source": [
    "total_mem_csv = 0\n",
    "with open(FILE_PATH, newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        total_mem_csv += get_memory_usage(row, framework=\"csv\")\n",
    "\n",
    "print(f\"CSV module total memory estimate: {total_mem_csv:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d895b601",
   "metadata": {},
   "source": [
    "# Method 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7bfaa5",
   "metadata": {},
   "source": [
    "Using the original `read_csv` from `polars`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6ff8ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polars original read_csv: 310.13 MB\n"
     ]
    }
   ],
   "source": [
    "df_polars = pl.read_csv(FILE_PATH)\n",
    "memory_polars = get_memory_usage(df_polars, framework=\"polars\")\n",
    "print(f\"Polars original read_csv: {memory_polars:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744a9305",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3166ef2",
   "metadata": {},
   "source": [
    "In this experiment, the goal was to parse a large CSV file, `customers-2000000.csv`, while keeping memory usage as low as possible. I compared several approaches and measured how much memory each method consumes.\n",
    "\n",
    "## Summary of Approaches and Memory Usage\n",
    "\n",
    "| Method | Description | Memory Usage (MB) |\n",
    "|--------|-------------|-----------------|\n",
    "| Pandas default `read_csv` | Loading the full CSV with default settings | 1506.04 |\n",
    "| Pandas chunking | Reading the CSV in 100k-row chunks | 1506.04 (per chunk ~75 MB) |\n",
    "| Pandas with optimized dtypes | Converting repeated strings to `category` and indexes to `int32` | 1010.90 |\n",
    "| CSV module | Reading the CSV manually row by row | 1220.70 |\n",
    "| Polars `read_csv` | Using Polars, designed for speed and low memory | 310.13 |\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "Parsing large CSV files can consume a lot of memory if not handled carefully. Default Pandas `read_csv` uses significant memory because all string columns are stored as `object` and indexes as `int64`. Chunking helps process the data in smaller pieces, but doesn’t reduce total memory usage. Optimizing dtypes in Pandas, by converting repeated strings to `category` and indexes to `int32`, can reduce memory by roughly a third. Manual CSV parsing is more memory-efficient than default Pandas but is slower and less convenient for large datasets. Polars, on the other hand, clearly outperforms all methods in memory efficiency, using only about 20% of the memory compared to Pandas default.\n",
    "\n",
    "For large datasets, **Polars** is the best choice when memory is a concern, being both fast and efficient. If prefer **Pandas**, always apply dtype optimization. Chunking is useful for step-by-step processing on limited-memory machines. Overall, careful selection of library and data types can drastically reduce memory usage, and for this dataset, **Polars is the most memory-efficient**, followed by **Pandas with optimized dtypes**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineer-technical-test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
